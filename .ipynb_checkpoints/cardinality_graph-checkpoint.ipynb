{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Column, Row\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Basic Cardinality Estimation\") \\\n",
    "    .config(\"spark.sql.cbo.enabled\", True) \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.cbo.joinReorder.enabled\", True) \\\n",
    "    .config(\"spark.sql.cbo.joinReorder.dp.threshold\", 16) \\\n",
    "    .config(\"spark.sql.statistics.histogram.enabled\", True) \\\n",
    "    .config(\"spark.sql.statistics.histogram.numBins\", 25) \\\n",
    "    .enableHiveSupport()\n",
    "\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_ints_df(max_val):\n",
    "    \"\"\"Returns a single column dataframe of 500 integers sampled uniformly from [0,50)\"\"\"\n",
    "    pd_df = pd.DataFrame(np.random.randint(0,max_val,[500], np.int))\n",
    "    df = spark.createDataFrame(pd_df, [\"x\"])\n",
    "    return df\n",
    "\n",
    "def create_uniform_tables():\n",
    "    \"\"\"Creates 4 tables with single column of uniform distribution and computes table and column statistics\"\"\"\n",
    "    table_names = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    for i in range(4):\n",
    "        table_name = table_names[i]\n",
    "        df = get_uniform_ints_df(25*(i+1))\n",
    "        df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "        spark.sql(\"Analyze Table \" + table_name + \" compute statistics\")\n",
    "        spark.sql(\"Analyze Table \" + table_name + \" compute statistics for columns \" + \"x\")\n",
    "        spark.sql(\"Describe extended \" + table_name).show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   a|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Fri Nov 30 19:17:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|2560 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   b|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Fri Nov 30 19:18:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|2970 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   c|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Fri Nov 30 19:18:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|3239 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   d|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Fri Nov 30 19:18:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|3396 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_uniform_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "stats[\"A\"] = spark.sql(\"describe extended A x\").rdd.collectAsMap()\n",
    "stats[\"B\"] = spark.sql(\"describe extended B x\").rdd.collectAsMap()\n",
    "stats[\"C\"] = spark.sql(\"describe extended C x\").rdd.collectAsMap()\n",
    "stats[\"D\"] = spark.sql(\"describe extended D x\").rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n",
      "371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_col_len': '8',\n",
       " 'bin_0': 'lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1',\n",
       " 'bin_1': 'lower_bound: 0.0, upper_bound: 2.0, distinct_count: 2',\n",
       " 'bin_10': 'lower_bound: 10.0, upper_bound: 11.0, distinct_count: 1',\n",
       " 'bin_11': 'lower_bound: 11.0, upper_bound: 12.0, distinct_count: 1',\n",
       " 'bin_12': 'lower_bound: 12.0, upper_bound: 13.0, distinct_count: 1',\n",
       " 'bin_13': 'lower_bound: 13.0, upper_bound: 13.0, distinct_count: 1',\n",
       " 'bin_14': 'lower_bound: 13.0, upper_bound: 14.0, distinct_count: 1',\n",
       " 'bin_15': 'lower_bound: 14.0, upper_bound: 15.0, distinct_count: 1',\n",
       " 'bin_16': 'lower_bound: 15.0, upper_bound: 16.0, distinct_count: 1',\n",
       " 'bin_17': 'lower_bound: 16.0, upper_bound: 17.0, distinct_count: 1',\n",
       " 'bin_18': 'lower_bound: 17.0, upper_bound: 18.0, distinct_count: 1',\n",
       " 'bin_19': 'lower_bound: 18.0, upper_bound: 20.0, distinct_count: 2',\n",
       " 'bin_2': 'lower_bound: 2.0, upper_bound: 3.0, distinct_count: 1',\n",
       " 'bin_20': 'lower_bound: 20.0, upper_bound: 20.0, distinct_count: 1',\n",
       " 'bin_21': 'lower_bound: 20.0, upper_bound: 22.0, distinct_count: 2',\n",
       " 'bin_22': 'lower_bound: 22.0, upper_bound: 22.0, distinct_count: 1',\n",
       " 'bin_23': 'lower_bound: 22.0, upper_bound: 23.0, distinct_count: 1',\n",
       " 'bin_24': 'lower_bound: 23.0, upper_bound: 24.0, distinct_count: 1',\n",
       " 'bin_3': 'lower_bound: 3.0, upper_bound: 4.0, distinct_count: 1',\n",
       " 'bin_4': 'lower_bound: 4.0, upper_bound: 5.0, distinct_count: 1',\n",
       " 'bin_5': 'lower_bound: 5.0, upper_bound: 6.0, distinct_count: 1',\n",
       " 'bin_6': 'lower_bound: 6.0, upper_bound: 7.0, distinct_count: 1',\n",
       " 'bin_7': 'lower_bound: 7.0, upper_bound: 8.0, distinct_count: 1',\n",
       " 'bin_8': 'lower_bound: 8.0, upper_bound: 9.0, distinct_count: 1',\n",
       " 'bin_9': 'lower_bound: 9.0, upper_bound: 10.0, distinct_count: 1',\n",
       " 'col_name': 'x',\n",
       " 'comment': 'NULL',\n",
       " 'data_type': 'bigint',\n",
       " 'distinct_count': '25',\n",
       " 'histogram': 'height: 20.0, num_of_bins: 25',\n",
       " 'max': '24',\n",
       " 'max_col_len': '8',\n",
       " 'min': '0',\n",
       " 'num_nulls': '0'}"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = spark.sql(\"Select * from D where D.x > 25\")\n",
    "print(result._jdf.queryExecution().optimizedPlan().stats().rowCount().get())\n",
    "print(result.count())\n",
    "stats[\"A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.demos import models\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "table_names = [\"A\", \"B\", \"C\", \"D\"]\n",
    "node_features = {}\n",
    "operators = [\"=\", \"<\", \">\", \"<=\", \">=\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_col_stats():\n",
    "    for table_name, column_stats in stats.items():\n",
    "        row_count = spark.sql(\"select * from \"+table_name).count()\n",
    "        feature = [row_count, \\\n",
    "                   column_stats['distinct_count'], \\\n",
    "                   column_stats['num_nulls'], \\\n",
    "                   column_stats['min'], \\\n",
    "                   column_stats['max'], \\\n",
    "                  column_stats['avg_col_len'], \\\n",
    "                  column_stats['max_col_len']]\n",
    "        feature_np = np.asarray(feature, dtype=np.float32)\n",
    "        #node_features[table_name] = tf.convert_to_tensor(feature_np, dtype=tf.float32) \n",
    "        node_features[table_name] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': [500, '25', '0', '0', '24', '8', '8'],\n",
       " 'B': [500, '52', '0', '0', '49', '8', '8'],\n",
       " 'C': [500, '76', '0', '0', '74', '8', '8'],\n",
       " 'D': [500, '101', '0', '0', '99', '8', '8']}"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_col_stats()\n",
    "node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dicts(rand, num_examples, selectivity=True):\n",
    "    \"\"\"Generates the input and target graphs for training.\n",
    "        Returns an array of input_graphs with featurized nodes\n",
    "        Without selectivity, 4 choose 2 = 6 possible graphs\n",
    "        With selectivity, generate 2 tables, and create a random selectivity on one of them\n",
    "        \n",
    "        Returns data dictionaries\"\"\"\n",
    "    input_graphs = []\n",
    "    estimated_cards = []\n",
    "    if not selectivity:\n",
    "        #Do not want to use selectivity when generating samples\n",
    "        #Total of 6 input graphs, ignore num_examples\n",
    "        for combo in list(itertools.combinations(range(4), 2)):\n",
    "            table1_name = table_names[combo[0]]\n",
    "            table2_name = table_names[combo[1]]\n",
    "            table1_features = [float(x) for x in node_features[table1_name]]\n",
    "            table2_features = [float(x) for x in node_features[table2_name]]\n",
    "            input_graphs.append({\"nodes\": [table1_features, table2_features], \"receivers\": [0, 1], \"senders\": [1, 0], \"edges\": [[0.0], [0.0]], \"globals\": [0.0]})\n",
    "            cardinality = spark.sql(\"select * from \"+table1_name+\",\"+table2_name+\" where \"+table1_name+\".x = \"+table2_name+\".x\") \\\n",
    "                .count()\n",
    "            estimated_cards.append(cardinality)\n",
    "        return input_graphs, estimated_cards\n",
    "    else:\n",
    "        #randomly generate num_examples number of 2 table joins with selections\n",
    "        for _ in range(num_examples):\n",
    "            #Randomly select 2 tables, and randomly generate selectivities\n",
    "            #tables = np.random.choice(table_names, size=2, replace=False)\n",
    "            tables = rand.choice(table_names, size=2, replace=False)\n",
    "            table1_name = tables[0]\n",
    "            table2_name = tables[1]\n",
    "            #table1_name = tf.gather(tables, 0)\n",
    "            #table2_name = tf.gather(tables, 1)\n",
    "            #print(table1_name, table2_name)\n",
    "            table1_features = [float(x) for x in node_features[table1_name]]\n",
    "            table2_features = [float(x) for x in node_features[table2_name]]\n",
    "            #operator = np.random.choice(operators)\n",
    "            operator = rand.choice(operators)\n",
    "            #val = np.random.randint(int(stats[table1_name]['max'])+1)\n",
    "            val = rand.randint(int(stats[table1_name]['max'])+1)\n",
    "            #print(operator, val)\n",
    "            query_result = spark.sql(\"Select * from \"+ table1_name + \" where \"+table1_name+\".x \"+operator+\" \"+str(val))\n",
    "            predicted_size = [query_result._jdf.queryExecution().optimizedPlan().stats().rowCount().get()]\n",
    "            #table1_features = tf.concat([table1_features, predicted_size], 0)\n",
    "            #table2_features = tf.concat([table2_features, [table2_features[0]]], 0)\n",
    "            #table1_features = np.append(table1_features, [predicted_size])\n",
    "            #table2_features = np.append(table2_features, [table2_features[0]])\n",
    "            table1_features = table1_features + predicted_size\n",
    "            table2_features = table2_features + [table2_features[0]]\n",
    "            input_graphs.append({\"nodes\": [table1_features, table2_features], \"receivers\": [0, 1], \"senders\": [1, 0], \"edges\": [[0.0], [0.0]], \"globals\": [0.0]})\n",
    "            cardinality = spark.sql(\"select * from \"+table1_name+\",\"+table2_name+\" where \"+table1_name+\".x = \"+table2_name+\".x \" \\\n",
    "                                   \"AND \"+table1_name+\".x \"+operator+\" \"+str(val)).count()\n",
    "            estimated_cards.append(cardinality)\n",
    "        return input_graphs, estimated_cards\n",
    "\n",
    "def create_target_dicts(batch_size, input_dicts, estimated_cards):\n",
    "    target_dicts = []\n",
    "    for i in range(batch_size):\n",
    "        cardinality = estimated_cards[i]\n",
    "        edges = [[cardinality], [cardinality]]\n",
    "        target_dict = input_dicts[i].copy()\n",
    "        target_dict['nodes'] = None\n",
    "        target_dict['edges'] = edges\n",
    "        target_dicts.append(target_dict)\n",
    "    return target_dicts\n",
    "# def create_target_graphs(batch_size, input_graphs, estimated_cards):\n",
    "#     target_graphs = []\n",
    "#     for i in range(batch_size):\n",
    "#         input_graph = utils_tf.get_graph(input_graphs, i)\n",
    "#         cardinality = estimated_cards[i]\n",
    "#         #table1 = relation_tuples[i][0]\n",
    "#         #table2 = relation_tuples[i][1]\n",
    "#         #cardinality = spark.sql(\"select * from \"+table1+\",\"+table2+\" where \"+table1+\".x = \"+table2+\".x\").count()\n",
    "#         num_edges = input_graph.n_edge\n",
    "#         edges = tf.constant([[cardinality], [cardinality]])\n",
    "#         #edges = [[cardinality], [cardinality]]\n",
    "#         target_graphs.append(input_graph._replace(edges=edges, nodes=None))\n",
    "#     return utils_tf.concat(target_graphs, axis=0)\n",
    "#     #return target_graphs\n",
    "\n",
    "\n",
    "def create_data_ops(rand, batch_size, selectivity=True):\n",
    "    if selectivity:\n",
    "        inputs_op, estimated_cards = create_graph_dicts(rand, batch_size, True)\n",
    "        targets_op = create_target_dicts(batch_size, inputs_op, estimated_cards)\n",
    "        #inputs_op = utils_tf.data_dicts_to_graphs_tuple(inputs_op)\n",
    "        #inputs_op = utils_tf.set_zero_edge_features(inputs_op, 1)\n",
    "        #inputs_op = utils_tf.set_zero_global_features(inputs_op, 1)\n",
    "        \n",
    "    else:\n",
    "        #ignore batch_size\n",
    "        inputs_op, estimated_cards = create_graph_dicts(rand, 6, False)\n",
    "        targets_op = create_target_dicts(6, inputs_op, estimated_cards)\n",
    "        #inputs_op = utils_tf.data_dicts_to_graphs_tuple(inputs_op)\n",
    "        #inputs_op = utils_tf.set_zero_edge_features(inputs_op, 1)\n",
    "        #inputs_op = utils_tf.set_zero_global_features(inputs_op, 1)\n",
    "    \n",
    "    \n",
    "    return inputs_op, targets_op\n",
    "\n",
    "def create_placeholders(rand, batch_size, selectivity=True):\n",
    "    \"\"\"Creates placeholders for the model training and evaluation\"\"\"\n",
    "    input_graphs, target_graphs = create_data_ops(rand, batch_size, selectivity)\n",
    "    #input_graphs = utils_tf.data_dicts_to_graphs_tuple(input_graphs)\n",
    "    #input_graphs = utils_tf.set_zero_edge_features(input_graphs, 1)\n",
    "    #input_graphs = utils_tf.set_zero_global_features(input_graphs, 1)\n",
    "    input_ph = utils_tf.placeholders_from_data_dicts(input_graphs, force_dynamic_num_graphs=True)\n",
    "    #input_ph = utils_tf._placeholders_from_graphs_tuple(input_graphs, force_dynamic_num_graphs=True)\n",
    "    target_ph = utils_tf.placeholders_from_data_dicts(target_graphs, force_dynamic_num_graphs=True)\n",
    "    #target_graphs = utils_tf.data_dicts_to_graphs_tuple(target_graphs)\n",
    "    #target_graphs = utils_tf.set_zero_node_features(target_graphs, 1)\n",
    "    #target_graphs = utils_tf.set_zero_global_features(target_graphs, 1)\n",
    "    #target_ph = utils_tf._placeholders_from_graphs_tuple(target_graphs, force_dynamic_num_graphs=True)\n",
    "    return input_ph, target_ph\n",
    "\n",
    "def create_feed_dict(rand, batch_size, selectivity, input_ph, target_ph):\n",
    "    inputs_graphs, target_graphs = create_data_ops(rand, batch_size, selectivity)\n",
    "#     inputs_op_tf, targets_op_tf = make_all_runnable_in_session(inputs_op_tf, targets_op_tf)\n",
    "#     inputs_graphs, targets_graphs = sess.run([inputs_op_tf, targets_op_tf])\n",
    "    input_graphs = utils_np.data_dicts_to_graphs_tuple(inputs_graphs)\n",
    "    #n_edges = np.sum(input_graphs.n_edge, axis=0)\n",
    "    #n_graphs = input_graphs.n_node.shape[0]\n",
    "    #input_graphs = utils_tf.set_zero_edge_features(input_graphs, 1)\n",
    "    #input_graphs = utils_tf.set_zero_global_features(input_graphs, 1)\n",
    "    target_graphs = utils_np.data_dicts_to_graphs_tuple(target_graphs)\n",
    "    #target_graphs = utils_tf.set_zero_node_features(target_graphs, 1)\n",
    "    #feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "    #print(input_ph)\n",
    "    feed_dict_input = utils_tf.get_feed_dict(input_ph, input_graphs)\n",
    "    feed_dict_target = utils_tf.get_feed_dict(target_ph, target_graphs)\n",
    "    feed_dict = {**feed_dict_input, **feed_dict_target}\n",
    "    return feed_dict\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "    return [utils_tf.make_runnable_in_session(a) for a in args]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'edges': [[0.0], [0.0]],\n",
       "   'globals': [0.0],\n",
       "   'nodes': [[500.0, 101.0, 0.0, 0.0, 99.0, 8.0, 8.0, 373],\n",
       "    [500.0, 76.0, 0.0, 0.0, 74.0, 8.0, 8.0, 500.0]],\n",
       "   'receivers': [0, 1],\n",
       "   'senders': [1, 0]},\n",
       "  {'edges': [[0.0], [0.0]],\n",
       "   'globals': [0.0],\n",
       "   'nodes': [[500.0, 101.0, 0.0, 0.0, 99.0, 8.0, 8.0, 376],\n",
       "    [500.0, 76.0, 0.0, 0.0, 74.0, 8.0, 8.0, 500.0]],\n",
       "   'receivers': [0, 1],\n",
       "   'senders': [1, 0]},\n",
       "  {'edges': [[0.0], [0.0]],\n",
       "   'globals': [0.0],\n",
       "   'nodes': [[500.0, 76.0, 0.0, 0.0, 74.0, 8.0, 8.0, 320],\n",
       "    [500.0, 25.0, 0.0, 0.0, 24.0, 8.0, 8.0, 500.0]],\n",
       "   'receivers': [0, 1],\n",
       "   'senders': [1, 0]}],\n",
       " [{'edges': [[2470], [2470]],\n",
       "   'globals': [0.0],\n",
       "   'nodes': None,\n",
       "   'receivers': [0, 1],\n",
       "   'senders': [1, 0]},\n",
       "  {'edges': [[2470], [2470]],\n",
       "   'globals': [0.0],\n",
       "   'nodes': None,\n",
       "   'receivers': [0, 1],\n",
       "   'senders': [1, 0]},\n",
       "  {'edges': [[3383], [3383]],\n",
       "   'globals': [0.0],\n",
       "   'nodes': None,\n",
       "   'receivers': [0, 1],\n",
       "   'senders': [1, 0]}])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "seed = 1  #@param{type: 'integer'}\n",
    "rand = np.random.RandomState(seed=seed)\n",
    "inputs_op_tf, targets_op_tf = create_data_ops(rand, 3, True)\n",
    "inputs_op_tf, targets_op_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Graphs:  GraphsTuple(nodes=array([[ 500.,   52.,    0.,    0.,   49.,    8.,    8.,   94.],\n",
      "       [ 500.,   25.,    0.,    0.,   24.,    8.,    8.,  500.],\n",
      "       [ 500.,   76.,    0.,    0.,   74.,    8.,    8.,    5.],\n",
      "       [ 500.,   25.,    0.,    0.,   24.,    8.,    8.,  500.],\n",
      "       [ 500.,   76.,    0.,    0.,   74.,    8.,    8.,    5.],\n",
      "       [ 500.,   52.,    0.,    0.,   49.,    8.,    8.,  500.]], dtype=float32), edges=array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]], dtype=float32), receivers=array([0, 1, 2, 3, 4, 5], dtype=int32), senders=array([1, 0, 3, 2, 5, 4], dtype=int32), globals=array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]], dtype=float32), n_node=array([2, 2, 2], dtype=int32), n_edge=array([2, 2, 2], dtype=int32))\n",
      "Target Graphs:  GraphsTuple(nodes=None, edges=array([[1749],\n",
      "       [1749],\n",
      "       [ 156],\n",
      "       [ 156],\n",
      "       [  44],\n",
      "       [  44]], dtype=int32), receivers=array([0, 1, 0, 1, 0, 1], dtype=int32), senders=array([1, 0, 1, 0, 1, 0], dtype=int32), globals=array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]], dtype=float32), n_node=array([0, 0, 0], dtype=int32), n_edge=array([2, 2, 2], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "input_ph, target_ph = create_placeholders(rand, 3, True)\n",
    "input_out, target_out = make_all_runnable_in_session(input_ph, target_ph)\n",
    "#print(input_ph, target_ph)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    feed_dict = create_feed_dict(rand, 3, True, input_ph, target_ph)\n",
    "    #print(feed_dict)\n",
    "    inputs_graphs, targets_graphs = sess.run([input_out, target_out], feed_dict=feed_dict)\n",
    "    print(\"Input Graphs: \",inputs_graphs)\n",
    "    print(\"Target Graphs: \", targets_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2714"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from B, C where B.x < 42 and B.x=C.x\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(target, output):\n",
    "    \"\"\"Computes symmetric mean absolute percentage error and solved accuracy of the final graph. Returns fraction of correctly solved cardinalities\n",
    "    \n",
    "        Target is the target graph\n",
    "        Output is the output graph\"\"\"\n",
    "    target_dicts = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    output_dicts = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    percent_difference = []\n",
    "    correctly_solved = []\n",
    "    for td, od in zip(target_dicts, output_dicts):\n",
    "        #predicted1 = od['edges'][0]\n",
    "        #predicted2 = od['edges'][1]\n",
    "        #actual1 = td['edges'][0]\n",
    "        #actual2 = td['edges'][1]\n",
    "        #num_edges = td['edges'].shape[0]\n",
    "        is_equal = np.equal(od['edges'], td['edges'])\n",
    "        numerator = np.absolute(od['edges']-td['edges'])\n",
    "        denom = np.absolute(od['edges']) + np.absolute(td['edges'])\n",
    "        #numerator = np.where(is_equal, [0, 0], numerator)\n",
    "        denom = np.where(is_equal, [1, 1], denom)\n",
    "        pd = np.mean(np.divide(numerator, denom))\n",
    "        #pd = np.mean(np.divide(np.absolute(od['edges']-td['edges']), np.absolute(td['edges']) + np.absolute(od['edges'])))\n",
    "        percent_difference.append(pd)\n",
    "        correctly_solved.append(np.equal(np.round(np.mean(od['edges'])), np.mean(td['edges'])))\n",
    "    correct = np.mean(correctly_solved)\n",
    "    pd = np.mean(percent_difference)\n",
    "    return pd, correct\n",
    "\n",
    "def create_loss_ops(target_op, output_ops):\n",
    "    if not isinstance(output_ops, collections.Sequence):\n",
    "        output_ops = [output_ops]\n",
    "    \n",
    "    loss_ops = [\n",
    "        tf.losses.mean_squared_error(target_op.edges, output_op.edges) for output_op in output_ops\n",
    "    ]\n",
    "    \n",
    "    return loss_ops\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amogkamsetty/anaconda3/envs/env_full_py3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "seed = 2\n",
    "rand = np.random.RandomState(seed=seed)\n",
    "\n",
    "num_processing_steps_tr = 10\n",
    "num_processing_steps_ge = 10\n",
    "\n",
    "num_training_iterations = 25000\n",
    "batch_size_tr = 32\n",
    "batch_size_ge = 100\n",
    "\n",
    "# inputs_op_tr, targets_op_tr = create_data_ops(\n",
    "#     batch_size_tr, True)\n",
    "# inputs_op_tr = utils_tf.set_zero_edge_features(inputs_op_tr, 1)\n",
    "# inputs_op_tr = utils_tf.set_zero_global_features(inputs_op_tr, 1)\n",
    "\n",
    "# inputs_op_ge, targets_op_ge = create_data_ops(\n",
    "#     batch_size_ge, True)\n",
    "# inputs_op_ge = utils_tf.set_zero_edge_features(inputs_op_ge, 1)\n",
    "# inputs_op_ge = utils_tf.set_zero_global_features(inputs_op_ge, 1)\n",
    "\n",
    "input_ph, target_ph = create_placeholders(rand, batch_size_tr, True)\n",
    "#input_ph = utils_tf.set_zero_edge_features(input_ph, 1)\n",
    "#input_ph = utils_tf.set_zero_global_features(input_ph, 1)\n",
    "\n",
    "# Instantiate the model.\n",
    "model = models.EncodeProcessDecode(edge_output_size=1, node_output_size=None)\n",
    "# A list of outputs, one per processing step.\n",
    "output_ops_tr = model(input_ph, num_processing_steps_tr)\n",
    "output_ops_ge = model(input_ph, num_processing_steps_ge)\n",
    "\n",
    "# Loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr  # loss_ops_tr\n",
    "loss_ops_ge = create_loss_ops(target_ph, output_ops_ge)\n",
    "loss_op_ge = loss_ops_ge[-1]\n",
    "\n",
    "# Optimizer.\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "# inputs_op_tr, targets_op_tr = make_all_runnable_in_session(\n",
    "#     inputs_op_tr, targets_op_tr)\n",
    "# inputs_op_ge, targets_op_ge = make_all_runnable_in_session(\n",
    "#     inputs_op_ge, targets_op_ge)\n",
    "\n",
    "input_out, target_out = make_all_runnable_in_session(input_ph, target_ph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "\n",
    "try:\n",
    "  sess.close()\n",
    "except NameError:\n",
    "  pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (test/generalization loss), Ctr (training fraction nodes/edges labeled correctly), Str (training fraction examples solved correctly), Cge (test/generalization fraction nodes/edges labeled correctly), Sge (test/generalization fraction examples solved correctly)\n",
      "# 00001, T 50.2, Ltr 4849508.0000, Lge 2908920.2500, Pdtr 0.9991, Ctr 0.1562, Pdge 0.9983, Cge 0.1300\n",
      "# 00002, T 76.3, Ltr 3777475.2500, Lge 2861543.0000, Pdtr 0.9990, Ctr 0.0938, Pdge 0.9974, Cge 0.0700\n",
      "# 00003, T 101.6, Ltr 3975669.2500, Lge 3185328.2500, Pdtr 0.9974, Ctr 0.2500, Pdge 0.9983, Cge 0.2200\n",
      "# 00004, T 126.9, Ltr 1538219.3750, Lge 3080652.2500, Pdtr 0.9971, Ctr 0.1250, Pdge 0.9979, Cge 0.1300\n",
      "# 00005, T 152.6, Ltr 3908976.7500, Lge 2634314.0000, Pdtr 0.9988, Ctr 0.1875, Pdge 0.9947, Cge 0.1700\n",
      "# 00006, T 178.8, Ltr 2504655.7500, Lge 3115052.7500, Pdtr 0.9981, Ctr 0.2500, Pdge 0.9950, Cge 0.1000\n",
      "# 00007, T 203.9, Ltr 2133226.2500, Lge 3080482.0000, Pdtr 0.9941, Ctr 0.2188, Pdge 0.9957, Cge 0.0700\n",
      "# 00008, T 229.3, Ltr 1626781.7500, Lge 3071339.5000, Pdtr 0.9927, Ctr 0.1562, Pdge 0.9950, Cge 0.0800\n",
      "# 00009, T 253.9, Ltr 1842219.3750, Lge 2685337.0000, Pdtr 0.9955, Ctr 0.1250, Pdge 0.9954, Cge 0.1400\n",
      "# 00010, T 279.2, Ltr 2179575.7500, Lge 3734110.5000, Pdtr 0.9979, Ctr 0.1875, Pdge 0.9975, Cge 0.1900\n",
      "# 00011, T 306.2, Ltr 2738116.0000, Lge 3623983.2500, Pdtr 0.9967, Ctr 0.0938, Pdge 0.9968, Cge 0.1000\n",
      "# 00012, T 331.3, Ltr 3266007.7500, Lge 2786300.7500, Pdtr 0.9957, Ctr 0.0625, Pdge 0.9945, Cge 0.1000\n",
      "# 00013, T 359.3, Ltr 3060555.2500, Lge 2532752.7500, Pdtr 0.9962, Ctr 0.0000, Pdge 0.9933, Cge 0.0300\n",
      "# 00014, T 383.6, Ltr 2761266.2500, Lge 2335436.2500, Pdtr 0.9944, Ctr 0.1250, Pdge 0.9924, Cge 0.0600\n",
      "# 00015, T 408.5, Ltr 3478084.7500, Lge 3446296.2500, Pdtr 0.9961, Ctr 0.0000, Pdge 0.9967, Cge 0.0400\n",
      "# 00016, T 432.3, Ltr 3148287.2500, Lge 3189049.0000, Pdtr 0.9940, Ctr 0.0000, Pdge 0.9936, Cge 0.0100\n",
      "# 00017, T 458.7, Ltr 2566991.2500, Lge 3981885.7500, Pdtr 0.9938, Ctr 0.0000, Pdge 0.9866, Cge 0.0100\n",
      "# 00018, T 485.5, Ltr 2735449.5000, Lge 3510387.2500, Pdtr 0.9941, Ctr 0.0000, Pdge 0.9939, Cge 0.0100\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33653.count.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:370)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:136)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:234)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:103)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.constructDoConsumeFunction(WholeStageCodegenExec.scala:211)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:182)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:184)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:206)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:184)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceBatches(ColumnarBatchScan.scala:144)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:83)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:98)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:238)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:164)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:581)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2831)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2830)\n\tat sun.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 17687.0 failed 1 times, most recent failure: Lost task 2.0 in stage 17687.0 (TID 52983, localhost, executor driver): java.io.FileNotFoundException: File file:/mnt/c/Users/abc/Documents/research/sparksql-graph/spark-warehouse/d/part-00001-3f13e99e-4089-42bb-997c-afc00389c12a-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:79)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:101)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: File file:/mnt/c/Users/abc/Documents/research/sparksql-graph/spark-warehouse/d/part-00001-3f13e99e-4089-42bb-997c-afc00389c12a-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\t... 3 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-387-5e963b422922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mlast_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#   input_graphs, target_graphs = create_data_ops(rand, batch_size_tr, True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#   input_graphs, target_graphs = sess.run([input_graphs, target_graphs])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-380-845b7e5952d3>\u001b[0m in \u001b[0;36mcreate_feed_dict\u001b[0;34m(rand, batch_size, selectivity, input_ph, target_ph)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselectivity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0minputs_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_graphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselectivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;31m#     inputs_op_tf, targets_op_tf = make_all_runnable_in_session(inputs_op_tf, targets_op_tf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m#     inputs_graphs, targets_graphs = sess.run([inputs_op_tf, targets_op_tf])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-380-845b7e5952d3>\u001b[0m in \u001b[0;36mcreate_data_ops\u001b[0;34m(rand, batch_size, selectivity)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_data_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselectivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mselectivity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0minputs_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimated_cards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mtargets_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_target_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimated_cards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m#inputs_op = utils_tf.data_dicts_to_graphs_tuple(inputs_op)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-380-845b7e5952d3>\u001b[0m in \u001b[0;36mcreate_graph_dicts\u001b[0;34m(rand, num_examples, selectivity)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtable2_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable2_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtable2_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0minput_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"nodes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtable1_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable2_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"receivers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"senders\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"edges\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"globals\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mcardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtable1_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtable2_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" where \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtable1_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".x = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtable2_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".x \"\u001b[0m                                    \u001b[0;34m\"AND \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtable1_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".x \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mestimated_cards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimated_cards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_full_py3/lib/python3.5/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_full_py3/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_full_py3/lib/python3.5/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_full_py3/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33653.count.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:370)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:136)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:234)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:103)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.constructDoConsumeFunction(WholeStageCodegenExec.scala:211)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:182)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:184)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:206)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:184)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceBatches(ColumnarBatchScan.scala:144)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:83)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:98)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:238)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:164)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:581)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2831)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2830)\n\tat sun.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 17687.0 failed 1 times, most recent failure: Lost task 2.0 in stage 17687.0 (TID 52983, localhost, executor driver): java.io.FileNotFoundException: File file:/mnt/c/Users/abc/Documents/research/sparksql-graph/spark-warehouse/d/part-00001-3f13e99e-4089-42bb-997c-afc00389c12a-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:306)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:79)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:101)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: File file:/mnt/c/Users/abc/Documents/research/sparksql-graph/spark-warehouse/d/part-00001-3f13e99e-4089-42bb-997c-afc00389c12a-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "  last_iteration = iteration\n",
    "  feed_dict = create_feed_dict(rand, batch_size_tr, True, input_ph, target_ph)\n",
    "#   input_graphs, target_graphs = create_data_ops(rand, batch_size_tr, True)\n",
    "#   input_graphs, target_graphs = sess.run([input_graphs, target_graphs])\n",
    "#   feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "  train_values = sess.run({\n",
    "      \"step\": step_op,\n",
    "      \"inputs\": input_out,\n",
    "      \"targets\": target_out,\n",
    "      \"loss\": loss_op_tr,\n",
    "      \"outputs\": output_ops_tr\n",
    "  }, feed_dict = feed_dict)\n",
    "  #print(\"Input Graphs: \",train_values[\"inputs\"])\n",
    "  the_time = time.time()\n",
    "  elapsed_since_last_log = the_time - last_log_time\n",
    "  if elapsed_since_last_log > log_every_seconds:\n",
    "    last_log_time = the_time\n",
    "    feed_dict = create_feed_dict(\n",
    "        rand, batch_size_ge, True, input_ph, target_ph)\n",
    "    test_values = sess.run({\n",
    "        \"targets\": target_out,\n",
    "        \"loss\": loss_op_ge,\n",
    "        \"outputs\": output_ops_ge,\n",
    "    }, feed_dict=feed_dict)\n",
    "    #print(train_values[\"targets\"])\n",
    "    #print(train_values[\"outputs\"])\n",
    "    pd_tr, correct_tr = compute_accuracy(train_values[\"targets\"],\n",
    "                                             train_values[\"outputs\"][-1])\n",
    "    pd_ge, correct_ge = compute_accuracy(test_values[\"targets\"],\n",
    "                                             test_values[\"outputs\"][-1])\n",
    "    elapsed = time.time() - start_time\n",
    "    losses_tr.append(train_values[\"loss\"])\n",
    "    corrects_tr.append(pd_tr)\n",
    "    solveds_tr.append(correct_tr)\n",
    "    losses_ge.append(test_values[\"loss\"])\n",
    "    solveds_ge.append(correct_ge)\n",
    "    corrects_ge.append(pd_ge)\n",
    "    logged_iterations.append(iteration)\n",
    "    print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Pdtr {:.4f}, \"\n",
    "          \"Ctr {:.4f}, Pdge {:.4f}, Cge {:.4f}\".format(\n",
    "              iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"],\n",
    "              pd_tr, correct_tr, pd_ge, correct_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Fraction examples solved')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60099e5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell visualizes the results of training. You can visualize the\n",
    "# intermediate results by interrupting execution of the cell above, and running\n",
    "# this cell. You can then resume training by simply executing the above cell\n",
    "# again.\n",
    "\n",
    "# Plot results curves.\n",
    "fig = plt.figure(11, figsize=(18, 3))\n",
    "fig.clf()\n",
    "x = np.array(logged_iterations)\n",
    "# Loss.\n",
    "y_tr = losses_tr\n",
    "y_ge = losses_ge\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"Loss across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"Loss (Mean Squared Error)\")\n",
    "ax.legend()\n",
    "# Correct.\n",
    "y_tr = corrects_tr\n",
    "y_ge = corrects_ge\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"SMAPE across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"SMAPE\")\n",
    "# Solved.\n",
    "y_tr = solveds_tr\n",
    "y_ge = solveds_ge\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"Fraction solved across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"Fraction examples solved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  [[ 4693.93652344]\n",
      " [ 4693.97558594]]\n",
      "Target:  [[4694]\n",
      " [4694]]\n",
      "Output:  [[ 3382.97216797]\n",
      " [ 3382.95166016]]\n",
      "Target:  [[3383]\n",
      " [3383]]\n",
      "Output:  [[ 2411.68725586]\n",
      " [ 2412.00805664]]\n",
      "Target:  [[2412]\n",
      " [2412]]\n",
      "Output:  [[ 3184.92895508]\n",
      " [ 3185.14599609]]\n",
      "Target:  [[3185]\n",
      " [3185]]\n",
      "Output:  [[ 2572.04199219]\n",
      " [ 2572.03466797]]\n",
      "Target:  [[2572]\n",
      " [2572]]\n",
      "Output:  [[ 2470.62939453]\n",
      " [ 2469.24609375]]\n",
      "Target:  [[2470]\n",
      " [2470]]\n"
     ]
    }
   ],
   "source": [
    "final_train_output = utils_np.graphs_tuple_to_data_dicts(train_values['outputs'][-1])\n",
    "final_target = utils_np.graphs_tuple_to_data_dicts(train_values['targets'])\n",
    "for out, target in zip(final_train_output, final_target):\n",
    "    print(\"Output: \",out['edges'])\n",
    "    print(\"Target: \", target['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
