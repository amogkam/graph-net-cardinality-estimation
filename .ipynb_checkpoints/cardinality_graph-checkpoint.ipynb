{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Column, Row\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Basic Cardinality Estimation\") \\\n",
    "    .config(\"spark.sql.cbo.enabled\", True) \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.cbo.joinReorder.enabled\", True) \\\n",
    "    .config(\"spark.sql.cbo.joinReorder.dp.threshold\", 16) \\\n",
    "    .config(\"spark.sql.statistics.histogram.enabled\", True) \\\n",
    "    .config(\"spark.sql.statistics.histogram.numBins\", 25) \\\n",
    "    .enableHiveSupport()\n",
    "\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_ints_df():\n",
    "    pd_df = pd.DataFrame(np.random.randint(0,50,[500], np.int))\n",
    "    df = spark.createDataFrame(pd_df, [\"x\"])\n",
    "    return df\n",
    "\n",
    "def create_uniform_tables():\n",
    "    #create 4 tables\n",
    "    table_names = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    for i in range(4):\n",
    "        table_name = table_names[i]\n",
    "        df = get_uniform_ints_df()\n",
    "        df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "        spark.sql(\"Analyze Table \" + table_name + \" compute statistics\")\n",
    "        spark.sql(\"Analyze Table \" + table_name + \" compute statistics for columns \" + \"x\")\n",
    "        spark.sql(\"Describe extended \" + table_name).show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   a|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Mon Nov 12 22:45:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|2986 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   b|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Mon Nov 12 22:45:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|2973 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   c|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Mon Nov 12 22:45:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|2962 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   d|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Mon Nov 12 22:45:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|2976 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_uniform_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "stats[\"A\"] = spark.sql(\"describe extended A x\").rdd.collectAsMap()\n",
    "stats[\"B\"] = spark.sql(\"describe extended B x\").rdd.collectAsMap()\n",
    "stats[\"C\"] = spark.sql(\"describe extended C x\").rdd.collectAsMap()\n",
    "stats[\"D\"] = spark.sql(\"describe extended D x\").rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_col_len': '8',\n",
       " 'bin_0': 'lower_bound: 0.0, upper_bound: 1.0, distinct_count: 2',\n",
       " 'bin_1': 'lower_bound: 1.0, upper_bound: 3.0, distinct_count: 2',\n",
       " 'bin_10': 'lower_bound: 19.0, upper_bound: 21.0, distinct_count: 2',\n",
       " 'bin_11': 'lower_bound: 21.0, upper_bound: 24.0, distinct_count: 3',\n",
       " 'bin_12': 'lower_bound: 24.0, upper_bound: 26.0, distinct_count: 2',\n",
       " 'bin_13': 'lower_bound: 26.0, upper_bound: 28.0, distinct_count: 2',\n",
       " 'bin_14': 'lower_bound: 28.0, upper_bound: 30.0, distinct_count: 2',\n",
       " 'bin_15': 'lower_bound: 30.0, upper_bound: 32.0, distinct_count: 2',\n",
       " 'bin_16': 'lower_bound: 32.0, upper_bound: 34.0, distinct_count: 2',\n",
       " 'bin_17': 'lower_bound: 34.0, upper_bound: 37.0, distinct_count: 3',\n",
       " 'bin_18': 'lower_bound: 37.0, upper_bound: 39.0, distinct_count: 2',\n",
       " 'bin_19': 'lower_bound: 39.0, upper_bound: 40.0, distinct_count: 1',\n",
       " 'bin_2': 'lower_bound: 3.0, upper_bound: 6.0, distinct_count: 3',\n",
       " 'bin_20': 'lower_bound: 40.0, upper_bound: 42.0, distinct_count: 2',\n",
       " 'bin_21': 'lower_bound: 42.0, upper_bound: 43.0, distinct_count: 1',\n",
       " 'bin_22': 'lower_bound: 43.0, upper_bound: 45.0, distinct_count: 2',\n",
       " 'bin_23': 'lower_bound: 45.0, upper_bound: 47.0, distinct_count: 2',\n",
       " 'bin_24': 'lower_bound: 47.0, upper_bound: 49.0, distinct_count: 2',\n",
       " 'bin_3': 'lower_bound: 6.0, upper_bound: 8.0, distinct_count: 2',\n",
       " 'bin_4': 'lower_bound: 8.0, upper_bound: 9.0, distinct_count: 1',\n",
       " 'bin_5': 'lower_bound: 9.0, upper_bound: 11.0, distinct_count: 2',\n",
       " 'bin_6': 'lower_bound: 11.0, upper_bound: 14.0, distinct_count: 3',\n",
       " 'bin_7': 'lower_bound: 14.0, upper_bound: 16.0, distinct_count: 2',\n",
       " 'bin_8': 'lower_bound: 16.0, upper_bound: 17.0, distinct_count: 1',\n",
       " 'bin_9': 'lower_bound: 17.0, upper_bound: 19.0, distinct_count: 2',\n",
       " 'col_name': 'x',\n",
       " 'comment': 'NULL',\n",
       " 'data_type': 'bigint',\n",
       " 'distinct_count': '52',\n",
       " 'histogram': 'height: 20.0, num_of_bins: 25',\n",
       " 'max': '49',\n",
       " 'max_col_len': '8',\n",
       " 'min': '0',\n",
       " 'num_nulls': '0'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = spark.sql(\"Select * from A where A.x > 25\")\n",
    "result.count()\n",
    "df_pandas = stats[\"A\"]\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
