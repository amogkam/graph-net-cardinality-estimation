{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Column, Row\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Basic Cardinality Estimation\") \\\n",
    "    .config(\"spark.sql.cbo.enabled\", True) \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.cbo.joinReorder.enabled\", True) \\\n",
    "    .config(\"spark.sql.cbo.joinReorder.dp.threshold\", 16) \\\n",
    "    .config(\"spark.sql.statistics.histogram.enabled\", True) \\\n",
    "    .config(\"spark.sql.statistics.histogram.numBins\", 25) \\\n",
    "    .enableHiveSupport()\n",
    "\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_ints_df(max_val):\n",
    "    \"\"\"Returns a single column dataframe of 500 integers sampled uniformly from [0,50)\"\"\"\n",
    "    pd_df = pd.DataFrame(np.random.randint(0,max_val,[500], np.int))\n",
    "    df = spark.createDataFrame(pd_df, [\"x\"])\n",
    "    return df\n",
    "\n",
    "def create_uniform_tables():\n",
    "    \"\"\"Creates 4 tables with single column of uniform distribution and computes table and column statistics\"\"\"\n",
    "    table_names = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    for i in range(4):\n",
    "        table_name = table_names[i]\n",
    "        df = get_uniform_ints_df(25*(i+1))\n",
    "        df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "        spark.sql(\"Analyze Table \" + table_name + \" compute statistics\")\n",
    "        spark.sql(\"Analyze Table \" + table_name + \" compute statistics for columns \" + \"x\")\n",
    "        spark.sql(\"Describe extended \" + table_name).show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   a|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Sun Nov 18 02:35:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|2560 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   b|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Sun Nov 18 02:35:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|2970 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   c|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Sun Nov 18 02:35:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|3239 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                   x|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|                   d|       |\n",
      "|               Owner|        amogkamsetty|       |\n",
      "|        Created Time|Sun Nov 18 02:35:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.4.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|3396 bytes, 500 rows|       |\n",
      "|            Location|file:/mnt/c/Users...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_uniform_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "stats[\"A\"] = spark.sql(\"describe extended A x\").rdd.collectAsMap()\n",
    "stats[\"B\"] = spark.sql(\"describe extended B x\").rdd.collectAsMap()\n",
    "stats[\"C\"] = spark.sql(\"describe extended C x\").rdd.collectAsMap()\n",
    "stats[\"D\"] = spark.sql(\"describe extended D x\").rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_col_len': '8',\n",
       " 'bin_0': 'lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1',\n",
       " 'bin_1': 'lower_bound: 0.0, upper_bound: 2.0, distinct_count: 2',\n",
       " 'bin_10': 'lower_bound: 10.0, upper_bound: 11.0, distinct_count: 1',\n",
       " 'bin_11': 'lower_bound: 11.0, upper_bound: 12.0, distinct_count: 1',\n",
       " 'bin_12': 'lower_bound: 12.0, upper_bound: 13.0, distinct_count: 1',\n",
       " 'bin_13': 'lower_bound: 13.0, upper_bound: 13.0, distinct_count: 1',\n",
       " 'bin_14': 'lower_bound: 13.0, upper_bound: 14.0, distinct_count: 1',\n",
       " 'bin_15': 'lower_bound: 14.0, upper_bound: 15.0, distinct_count: 1',\n",
       " 'bin_16': 'lower_bound: 15.0, upper_bound: 16.0, distinct_count: 1',\n",
       " 'bin_17': 'lower_bound: 16.0, upper_bound: 17.0, distinct_count: 1',\n",
       " 'bin_18': 'lower_bound: 17.0, upper_bound: 18.0, distinct_count: 1',\n",
       " 'bin_19': 'lower_bound: 18.0, upper_bound: 20.0, distinct_count: 2',\n",
       " 'bin_2': 'lower_bound: 2.0, upper_bound: 3.0, distinct_count: 1',\n",
       " 'bin_20': 'lower_bound: 20.0, upper_bound: 20.0, distinct_count: 1',\n",
       " 'bin_21': 'lower_bound: 20.0, upper_bound: 22.0, distinct_count: 2',\n",
       " 'bin_22': 'lower_bound: 22.0, upper_bound: 22.0, distinct_count: 1',\n",
       " 'bin_23': 'lower_bound: 22.0, upper_bound: 23.0, distinct_count: 1',\n",
       " 'bin_24': 'lower_bound: 23.0, upper_bound: 24.0, distinct_count: 1',\n",
       " 'bin_3': 'lower_bound: 3.0, upper_bound: 4.0, distinct_count: 1',\n",
       " 'bin_4': 'lower_bound: 4.0, upper_bound: 5.0, distinct_count: 1',\n",
       " 'bin_5': 'lower_bound: 5.0, upper_bound: 6.0, distinct_count: 1',\n",
       " 'bin_6': 'lower_bound: 6.0, upper_bound: 7.0, distinct_count: 1',\n",
       " 'bin_7': 'lower_bound: 7.0, upper_bound: 8.0, distinct_count: 1',\n",
       " 'bin_8': 'lower_bound: 8.0, upper_bound: 9.0, distinct_count: 1',\n",
       " 'bin_9': 'lower_bound: 9.0, upper_bound: 10.0, distinct_count: 1',\n",
       " 'col_name': 'x',\n",
       " 'comment': 'NULL',\n",
       " 'data_type': 'bigint',\n",
       " 'distinct_count': '25',\n",
       " 'histogram': 'height: 20.0, num_of_bins: 25',\n",
       " 'max': '24',\n",
       " 'max_col_len': '8',\n",
       " 'min': '0',\n",
       " 'num_nulls': '0'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = spark.sql(\"Select * from A where A.x > 25\")\n",
    "print(result._jdf.queryExecution().optimizedPlan().stats().rowCount().get())\n",
    "result.count()\n",
    "stats[\"A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.demos import models\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "table_names = [\"A\", \"B\", \"C\", \"D\"]\n",
    "node_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_col_stats():\n",
    "    for table_name, column_stats in stats.items():\n",
    "        row_count = spark.sql(\"select * from \"+table_name).count()\n",
    "        feature = [row_count, \\\n",
    "                   column_stats['distinct_count'], \\\n",
    "                   column_stats['num_nulls'], \\\n",
    "                   column_stats['min'], \\\n",
    "                   column_stats['max'], \\\n",
    "                  column_stats['avg_col_len'], \\\n",
    "                  column_stats['max_col_len']]\n",
    "        feature_np = np.asarray(feature, dtype=np.int32)\n",
    "        node_features[table_name] = tf.convert_to_tensor(feature_np, dtype=tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': <tf.Tensor 'Const_22:0' shape=(7,) dtype=float32>,\n",
       " 'B': <tf.Tensor 'Const_25:0' shape=(7,) dtype=float32>,\n",
       " 'C': <tf.Tensor 'Const_24:0' shape=(7,) dtype=float32>,\n",
       " 'D': <tf.Tensor 'Const_23:0' shape=(7,) dtype=float32>}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_col_stats()\n",
    "node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dicts(num_examples, selectivity=True):\n",
    "    \"\"\"Generates the input and target graphs for training.\n",
    "        Returns an array of input_graphs with featurized nodes\n",
    "        Without selectivity, 4 choose 2 = 6 possible graphs\n",
    "        With selectivity, generate 2 tables, and create a random selectivity on one of them\"\"\"\n",
    "    input_graphs = []\n",
    "    relation_tuples = []\n",
    "    if not selectivity:\n",
    "        #Do not want to use selectivity when generating samples\n",
    "        #Total of 6 input graphs, ignore num_examples\n",
    "        for combo in list(itertools.combinations(range(4), 2)):\n",
    "            table1_name = table_names[combo[0]]\n",
    "            table2_name = table_names[combo[1]]\n",
    "            table1_features = node_features[table1_name]\n",
    "            table2_features = node_features[table2_name]\n",
    "            input_graphs.append({\"nodes\": [table1_features, table2_features], \"receivers\": [0, 1], \"senders\": [1, 0]})\n",
    "            relation_tuples.append(combo)\n",
    "        return input_graphs, relation_tuples\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def create_target_graphs(batch_size, input_graphs, relation_tuples):\n",
    "    target_graphs = []\n",
    "    for i in range(batch_size):\n",
    "        input_graph = utils_tf.get_graph(input_graphs, i)\n",
    "        table1 = relation_tuples[i][0]\n",
    "        table2 = relation_tuples[i][1]\n",
    "        cardinality = spark.sql(\"select * from \"+table_names[table1]+\",\"+table_names[table2]+\" where \"+table_names[table1]+\".x = \"+table_names[table2]+\".x\").count()\n",
    "        num_edges = input_graph.n_edge\n",
    "        edges = tf.constant([[cardinality], [cardinality]])\n",
    "        target_graphs.append(input_graph._replace(edges=edges, nodes=None))\n",
    "    return utils_tf.concat(target_graphs, axis=0)\n",
    "\n",
    "def create_data_ops(batch_size, selectivity=True):\n",
    "    if selectivity:\n",
    "        pass\n",
    "    else:\n",
    "        #ignore batch_size\n",
    "        inputs_op, relation_tuples = create_graph_dicts(6, False)\n",
    "        inputs_op = utils_tf.data_dicts_to_graphs_tuple(inputs_op)\n",
    "        #inputs_op = utils_tf.fully_connect_graph_dynamic(inputs_op)\n",
    "    \n",
    "        targets_op = create_target_graphs(6, inputs_op, relation_tuples)\n",
    "    \n",
    "    return inputs_op, targets_op\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "    return [utils_tf.make_runnable_in_session(a) for a in args]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GraphsTuple(nodes=<tf.Tensor 'data_dicts_to_graphs_tuple_12/concat_1:0' shape=(12, 7) dtype=float32>, edges=None, receivers=<tf.Tensor 'data_dicts_to_graphs_tuple_12/add:0' shape=(12,) dtype=int32>, senders=<tf.Tensor 'data_dicts_to_graphs_tuple_12/add_1:0' shape=(12,) dtype=int32>, globals=None, n_node=<tf.Tensor 'data_dicts_to_graphs_tuple_12/stack_1:0' shape=(6,) dtype=int32>, n_edge=<tf.Tensor 'data_dicts_to_graphs_tuple_12/stack:0' shape=(6,) dtype=int32>),\n",
       " GraphsTuple(nodes=None, edges=<tf.Tensor 'graph_concat_10/concat_edges:0' shape=(12, 1) dtype=int32>, receivers=<tf.Tensor 'graph_concat_10/add:0' shape=(12,) dtype=int32>, senders=<tf.Tensor 'graph_concat_10/add_1:0' shape=(12,) dtype=int32>, globals=None, n_node=<tf.Tensor 'graph_concat_10/concat_n_node:0' shape=(6,) dtype=int32>, n_edge=<tf.Tensor 'graph_concat_10/concat_n_edge:0' shape=(6,) dtype=int32>))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "inputs_op_tf, targets_op_tf = create_data_ops(6, False)\n",
    "inputs_op_tf, targets_op_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Graphs:  GraphsTuple(nodes=array([[ 500.,   25.,    0.,    0.,   24.,    8.,    8.],\n",
      "       [ 500.,   52.,    0.,    0.,   49.,    8.,    8.],\n",
      "       [ 500.,   25.,    0.,    0.,   24.,    8.,    8.],\n",
      "       [ 500.,   76.,    0.,    0.,   74.,    8.,    8.],\n",
      "       [ 500.,   25.,    0.,    0.,   24.,    8.,    8.],\n",
      "       [ 500.,  101.,    0.,    0.,   99.,    8.,    8.],\n",
      "       [ 500.,   52.,    0.,    0.,   49.,    8.,    8.],\n",
      "       [ 500.,   76.,    0.,    0.,   74.,    8.,    8.],\n",
      "       [ 500.,   52.,    0.,    0.,   49.,    8.,    8.],\n",
      "       [ 500.,  101.,    0.,    0.,   99.,    8.,    8.],\n",
      "       [ 500.,   76.,    0.,    0.,   74.,    8.,    8.],\n",
      "       [ 500.,  101.,    0.,    0.,   99.,    8.,    8.]], dtype=float32), edges=None, receivers=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32), senders=array([ 1,  0,  3,  2,  5,  4,  7,  6,  9,  8, 11, 10], dtype=int32), globals=None, n_node=array([2, 2, 2, 2, 2, 2], dtype=int32), n_edge=array([2, 2, 2, 2, 2, 2], dtype=int32))\n",
      "Target Graphs:  GraphsTuple(nodes=None, edges=array([[4694],\n",
      "       [4694],\n",
      "       [3383],\n",
      "       [3383],\n",
      "       [2412],\n",
      "       [2412],\n",
      "       [3185],\n",
      "       [3185],\n",
      "       [2572],\n",
      "       [2572],\n",
      "       [2470],\n",
      "       [2470]], dtype=int32), receivers=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32), senders=array([ 1,  0,  3,  2,  5,  4,  7,  6,  9,  8, 11, 10], dtype=int32), globals=None, n_node=array([2, 2, 2, 2, 2, 2], dtype=int32), n_edge=array([2, 2, 2, 2, 2, 2], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "inputs_op_tf, targets_op_tf = make_all_runnable_in_session(inputs_op_tf, targets_op_tf)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    inputs_graphs, targets_graphs = sess.run([inputs_op_tf, targets_op_tf])\n",
    "    print(\"Input Graphs: \",inputs_graphs)\n",
    "    print(\"Target Graphs: \", targets_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(target, output):\n",
    "    \"\"\"Computes accuracy of the final graph. Returns fraction of correctly solved cardinalities\n",
    "    \n",
    "        Target is the target graph\n",
    "        Output is the output graph\"\"\"\n",
    "    target_dicts = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    output_dicts = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    correctly_solved = []\n",
    "    for td, od in zip(target_dicts, output_dicts):\n",
    "        predicted1 = od['edges'][0]\n",
    "        predicted2 = od['edges'][1]\n",
    "        actual1 = td['edges'][0]\n",
    "        actual2 = td['edges'][1]\n",
    "        if predicted1==actual1 and predicted2==actual2:\n",
    "            correctly_solved.append(1)\n",
    "        else:\n",
    "            correctly_solved.append(0)\n",
    "    correct = np.mean(correctly_solved)\n",
    "    return correct\n",
    "\n",
    "def create_loss_ops(target_op, output_ops):\n",
    "    if not isinstance(output_ops, collections.Sequence):\n",
    "        output_ops = [output_ops]\n",
    "    \n",
    "    loss_ops = [\n",
    "        tf.losses.mean_squared_error(target_op.edges, output_op.edges) for output_op in output_ops\n",
    "    ]\n",
    "    \n",
    "    return loss_ops\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "num_processing_steps_tr = 10\n",
    "num_processing_steps_ge = 10\n",
    "\n",
    "num_training_iterations = 10000000\n",
    "batch_size_tr = 6\n",
    "batch_size_ge = 6\n",
    "\n",
    "inputs_op_tr, targets_op_tr = create_data_ops(\n",
    "    batch_size_tr, False)\n",
    "inputs_op_tr = utils_tf.set_zero_edge_features(inputs_op_tr, 1)\n",
    "inputs_op_tr = utils_tf.set_zero_global_features(inputs_op_tr, 1)\n",
    "#target_op_tr = utils_tf.set_zero_node_features(targets_op_tr, 1)\n",
    "\n",
    "inputs_op_ge, targets_op_ge = create_data_ops(\n",
    "    batch_size_ge, False)\n",
    "inputs_op_ge = utils_tf.set_zero_edge_features(inputs_op_ge, 1)\n",
    "inputs_op_ge = utils_tf.set_zero_global_features(inputs_op_ge, 1)\n",
    "#target_op_ge = utils_tf.set_zero_node_features(targets_op_ge, 1)\n",
    "\n",
    "# Instantiate the model.\n",
    "model = models.EncodeProcessDecode(edge_output_size=1, node_output_size=None)\n",
    "# A list of outputs, one per processing step.\n",
    "output_ops_tr = model(inputs_op_tr, num_processing_steps_tr)\n",
    "output_ops_ge = model(inputs_op_ge, num_processing_steps_ge)\n",
    "\n",
    "# Loss.\n",
    "loss_ops_tr = create_loss_ops(targets_op_tr, output_ops_tr)\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr  # loss_ops_tr\n",
    "loss_ops_ge = create_loss_ops(targets_op_ge, output_ops_ge)\n",
    "loss_op_ge = loss_ops_ge[-1]\n",
    "\n",
    "# Optimizer.\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "inputs_op_tr, targets_op_tr = make_all_runnable_in_session(\n",
    "    inputs_op_tr, targets_op_tr)\n",
    "inputs_op_ge, targets_op_ge = make_all_runnable_in_session(\n",
    "    inputs_op_ge, targets_op_ge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "\n",
    "try:\n",
    "  sess.close()\n",
    "except NameError:\n",
    "  pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (test/generalization loss), Ctr (training fraction nodes/edges labeled correctly), Str (training fraction examples solved correctly), Cge (test/generalization fraction nodes/edges labeled correctly), Sge (test/generalization fraction examples solved correctly)\n",
      "# 01436, T 22.0, Ltr 9331994.0000, Lge 9330773.0000, Ctr 0.0000, Cge 0.0000\n",
      "# 03353, T 40.0, Ltr 6370661.0000, Lge 6368956.0000, Ctr 0.0000, Cge 0.0000\n",
      "# 05481, T 60.0, Ltr 3008800.2500, Lge 3007464.7500, Ctr 0.0000, Cge 0.0000\n",
      "# 07578, T 80.0, Ltr 1052513.3750, Lge 1052007.0000, Ctr 0.0000, Cge 0.0000\n",
      "# 09642, T 100.0, Ltr 349020.5938, Lge 348837.3125, Ctr 0.0000, Cge 0.0000\n",
      "# 11680, T 120.0, Ltr 68396.0625, Lge 68312.1719, Ctr 0.0000, Cge 0.0000\n",
      "# 13790, T 140.0, Ltr 791.1899, Lge 788.3756, Ctr 0.0000, Cge 0.0000\n",
      "# 15920, T 160.0, Ltr 0.0169, Lge 0.0021, Ctr 0.0000, Cge 0.0000\n",
      "# 18030, T 180.1, Ltr 7.8798, Lge 0.2998, Ctr 0.0000, Cge 0.0000\n",
      "# 20121, T 200.1, Ltr 0.0820, Lge 0.0145, Ctr 0.0000, Cge 0.0000\n",
      "# 22222, T 220.1, Ltr 0.1252, Lge 0.0684, Ctr 0.0000, Cge 0.0000\n",
      "# 24346, T 240.1, Ltr 0.0091, Lge 0.0057, Ctr 0.0000, Cge 0.0000\n",
      "# 26463, T 260.1, Ltr 425979.0000, Lge 409357.7500, Ctr 0.0000, Cge 0.0000\n",
      "# 28590, T 280.1, Ltr 2.8066, Lge 0.5316, Ctr 0.0000, Cge 0.0000\n",
      "# 30714, T 300.1, Ltr 1.1296, Lge 0.0904, Ctr 0.0000, Cge 0.0000\n",
      "# 32789, T 320.1, Ltr 0.5074, Lge 0.1584, Ctr 0.0000, Cge 0.0000\n",
      "# 34904, T 340.1, Ltr 2.2837, Lge 11.5499, Ctr 0.0000, Cge 0.0000\n",
      "# 36944, T 360.1, Ltr 0.3262, Lge 0.0554, Ctr 0.0000, Cge 0.0000\n",
      "# 38583, T 380.1, Ltr 0.1248, Lge 0.0330, Ctr 0.0000, Cge 0.0000\n",
      "# 40579, T 400.1, Ltr 0.0465, Lge 0.0097, Ctr 0.0000, Cge 0.0000\n",
      "# 42227, T 420.1, Ltr 0.0491, Lge 0.0062, Ctr 0.0000, Cge 0.0000\n"
     ]
    }
   ],
   "source": [
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "  last_iteration = iteration\n",
    "  train_values = sess.run({\n",
    "      \"step\": step_op,\n",
    "      \"inputs\": inputs_op_tr,\n",
    "      \"targets\": targets_op_tr,\n",
    "      \"loss\": loss_op_tr,\n",
    "      \"outputs\": output_ops_tr\n",
    "  })\n",
    "  the_time = time.time()\n",
    "  elapsed_since_last_log = the_time - last_log_time\n",
    "  if elapsed_since_last_log > log_every_seconds:\n",
    "    last_log_time = the_time\n",
    "    test_values = sess.run({\n",
    "        \"targets\": targets_op_ge,\n",
    "        \"loss\": loss_op_ge,\n",
    "        \"outputs\": output_ops_ge,\n",
    "    })\n",
    "    #print(train_values[\"targets\"])\n",
    "    #print(train_values[\"outputs\"])\n",
    "    correct_tr = compute_accuracy(train_values[\"targets\"],\n",
    "                                             train_values[\"outputs\"][-1])\n",
    "    correct_ge = compute_accuracy(test_values[\"targets\"],\n",
    "                                             test_values[\"outputs\"][-1])\n",
    "    elapsed = time.time() - start_time\n",
    "    losses_tr.append(train_values[\"loss\"])\n",
    "    corrects_tr.append(correct_tr)\n",
    "    losses_ge.append(test_values[\"loss\"])\n",
    "    corrects_ge.append(correct_ge)\n",
    "    logged_iterations.append(iteration)\n",
    "    print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, \"\n",
    "          \"Cge {:.4f}\".format(\n",
    "              iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"],\n",
    "              correct_tr, correct_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
